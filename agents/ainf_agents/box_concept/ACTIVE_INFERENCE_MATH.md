# Active Inference for 3D Box Estimation

## Mathematical Formalism

This document describes the complete mathematical framework for estimating 3D box obstacles using Active Inference, as implemented in the `box_manager.py` module.

---

## 1. Problem Statement

A mobile robot equipped with a 3D LIDAR sensor observes its environment and must infer the position, orientation, and dimensions of box-shaped obstacles. The robot's pose in the room frame is uncertain, characterized by a covariance matrix. The goal is to maintain probabilistic beliefs over box parameters that are updated as new observations arrive.

---

## 2. State Space

### 2.1 Box State Vector

Each box is parameterized by a 6-dimensional state vector:

$$
\mathbf{s} = (c_x, c_y, w, h, d, \theta)^\top
$$

where:
- $(c_x, c_y)$: center position in room frame (meters)
- $w$: width (meters)
- $h$: height/length in XY plane (meters)  
- $d$: depth/vertical extent (meters)
- $\theta$: orientation angle (radians)

### 2.2 Robot State

The robot pose in the room frame:

$$
\mathbf{r}_t = (r_x, r_y, r_\theta)^\top
$$

with associated covariance:

$$
\Sigma_{\text{robot}}(t) \in \mathbb{R}^{3 \times 3}
$$

---

## 3. Generative Model

### 3.1 Observation Model

LIDAR points are generated by the surfaces of boxes. For a point $\mathbf{p}$ observed in the robot frame, we transform it to the room frame:

$$
\mathbf{p}_{\text{room}} = R(r_\theta) \mathbf{p}_{\text{robot}} + \begin{pmatrix} r_x \\ r_y \end{pmatrix}
$$

The generative model assumes that observed surface points have zero signed distance to the box surface, corrupted by Gaussian noise:

$$
p(\mathbf{p} | \mathbf{s}) = \mathcal{N}\left( \text{SDF}(\mathbf{p}, \mathbf{s}) \mid 0, \sigma_o^2 \right)
$$

where $\text{SDF}(\mathbf{p}, \mathbf{s})$ is the signed distance function.

### 3.2 Signed Distance Function for 3D Box

The SDF for a 3D oriented box is computed as follows:

1. **Transform point to box-local frame:**
$$
\mathbf{p}_{\text{local}} = R(-\theta)^\top (\mathbf{p} - \mathbf{c})
$$

2. **Compute distance to box surface:**
$$
\mathbf{q} = |\mathbf{p}_{\text{local}}| - \frac{1}{2}\begin{pmatrix} w \\ h \\ d \end{pmatrix}
$$

3. **SDF value (standard formulation):**
$$
\text{SDF}(\mathbf{p}, \mathbf{s}) = \|\max(\mathbf{q}, 0)\| + \min(\max(q_x, q_y, q_z), 0)
$$

The first term handles points outside the box, the second handles points inside.

### 3.2.1 Smooth Minimum for Internal Points

**Problem:** The standard SDF uses a hard minimum for internal points: $\min(\max(q_x, q_y, q_z), 0)$. This means gradients only flow to the dimension with the smallest (most negative) distance. When LIDAR only sees some faces of the box, dimensions corresponding to unseen faces receive zero gradient and cannot be optimized.

**Solution:** We replace the hard minimum with a **smooth minimum** using log-sum-exp:

$$
\text{smooth\_min}(q_x, q_y, q_z) \approx -k \cdot \log\left( e^{-q_x/k} + e^{-q_y/k} + e^{-q_z/k} \right)
$$

where $k$ is a smoothness parameter (default $k = 0.02$ m).

**Properties:**
- As $k \to 0$: approaches hard minimum
- Larger $k$: smoother gradients, all dimensions receive gradient updates
- Preserves correct SDF behavior at the surface

**Additionally:** Internal points are scaled by a factor $\alpha_{\text{inside}} = 0.5$ to reduce their influence, as they are less reliable than surface points.

**Final SDF for internal points:**
$$
\text{SDF}_{\text{inside}} = \alpha_{\text{inside}} \cdot \text{smooth\_min}(q_x, q_y, q_z)
$$

**Configuration constants (in `object_sdf_prior.py`):**
| Constant | Description | Default |
|----------|-------------|---------|
| `SDF_SMOOTH_K` | Smoothness parameter $k$ | 0.02 m |
| `SDF_INSIDE_SCALE` | Internal point scale $\alpha_{\text{inside}}$ | 0.5 |

### 3.3 Prior Model (Temporal Dynamics)

Boxes are assumed static. The prior at time $t$ is the posterior from time $t-1$ with added process noise:

$$
p(\mathbf{s}_t) = \mathcal{N}(\mathbf{s}_t \mid \mathbf{\mu}_{t-1}, \Sigma_{t-1} + \Sigma_{\text{process}})
$$

where:
$$
\Sigma_{\text{process}} = \text{diag}(\sigma_{xy}^2, \sigma_{xy}^2, \sigma_{\text{size}}^2, \sigma_{\text{size}}^2, \sigma_{\text{size}}^2, \sigma_{\theta}^2)
$$

### 3.4 Prior for Box Dimensions

New boxes are initialized with a prior favoring typical box dimensions:

$$
p(w), p(h), p(d) \sim \mathcal{N}(\mu_{\text{size}}, \sigma_{\text{size}}^2)
$$

with $\mu_{\text{size}} = 0.5$ m (typical box size).

### 3.5 Prior for Angle Alignment

Boxes in indoor environments tend to align with room axes (walls). We model this with a **mixture of Gaussians prior** centered at aligned angles:

$$
p(\theta) \propto \sum_{k \in \{0, \pm\pi/2\}} \exp\left( -\frac{(\theta - \mu_k)^2}{2\sigma_\theta^2} \right)
$$

where:
- $\mu_k \in \{0, \pi/2, -\pi/2\}$: aligned angles (0°, 90°, -90°)
- $\sigma_\theta$: standard deviation (default 0.1 rad ≈ 6°)
- $\lambda_\theta = 1/\sigma_\theta^2$: precision

**Prior Energy Term:**

$$
E_\theta(\theta) = \frac{\lambda_\theta}{2} \min_k (\theta - \mu_k)^2
$$

This term:
- **Equals zero** when $\theta$ is aligned (0° or 90°)
- **Increases quadratically** as $\theta$ deviates from alignment
- Acts as a **soft constraint** that biases boxes toward room-aligned orientations

**Effect on VFE:**

The total VFE becomes:

$$
F = F_{\text{likelihood}} + F_{\text{state prior}} + \gamma \cdot E_\theta(\theta)
$$

where $\gamma$ is the weight for the angle alignment prior (default 0.5).

---

## 4. Variational Free Energy

### 4.1 Definition

Following the Active Inference framework, we approximate the posterior $p(\mathbf{s} | \mathbf{o})$ with a Gaussian recognition density:

$$
q(\mathbf{s}) = \mathcal{N}(\mathbf{s} \mid \mathbf{\mu}, \Sigma)
$$

The Variational Free Energy (VFE) to minimize is:

$$
F[q, \mathbf{o}] = D_{KL}[q(\mathbf{s}) \| p(\mathbf{s})] - \mathbb{E}_q[\ln p(\mathbf{o} | \mathbf{s})]
$$

### 4.2 Decomposition

$$
F = \underbrace{D_{KL}[q(\mathbf{s}) \| p(\mathbf{s})]}_{\text{Complexity}} - \underbrace{\mathbb{E}_q[\ln p(\mathbf{o} | \mathbf{s})]}_{\text{Accuracy}}
$$

- **Complexity**: Penalizes deviation from prior (Occam's razor)
- **Accuracy**: Rewards explaining observations

### 4.3 Laplace Approximation

Under Gaussian assumptions and point estimation at the mode:

$$
F(\mathbf{s}) \approx \underbrace{\frac{1}{2\sigma_o^2} \sum_{i=1}^{N} \text{SDF}(\mathbf{p}_i, \mathbf{s})^2}_{\text{Likelihood (prediction error)}} + \underbrace{\frac{1}{2} (\mathbf{s} - \mathbf{\mu}_{\text{prior}})^\top \Sigma_{\text{prior}}^{-1} (\mathbf{s} - \mathbf{\mu}_{\text{prior}})}_{\text{Prior regularization}}
$$

---

## 5. Historical Points and Evidence Accumulation

### 5.1 Motivation

Single-frame observations only capture visible surfaces. To build a complete model, we accumulate evidence from multiple viewpoints as the robot moves.

### 5.2 Historical Point Storage

Each historical point is stored with:
- Position in room frame: $\mathbf{p}_{\text{room}} \in \mathbb{R}^3$
- Capture covariance: $\Sigma_{\text{capture}}$
- RFE score: accumulated consistency measure

#### Capture Covariance Formulation

The capture covariance combines two independent sources of uncertainty:

$
\Sigma_{\text{capture}} = \Sigma_{\text{robot}}(t_0) + \sigma_{\text{sdf}}^2 \mathbf{I}
$

where:
- $\Sigma_{\text{robot}}(t_0)$: Robot localization uncertainty at capture time
- $\sigma_{\text{sdf}}^2 = \beta \cdot \text{SDF}(\mathbf{p}, \mathbf{s})^2$: Measurement uncertainty from SDF

**Physical interpretation:**
- $\Sigma_{\text{robot}}(t_0)$: "How well localized was the robot when this point was captured?"
- $\sigma_{\text{sdf}}^2$: "How well did this point fit the model at capture time?"

These are **independent error sources**, hence they add:

$
\sigma_{\text{sdf}}^2 = \beta \cdot \text{SDF}(\mathbf{p}, \mathbf{s}_{t_0})^2
$

The SDF value at capture time modulates the point's reliability:
- $\text{SDF} \approx 0$: Point exactly on surface → low $\sigma_{\text{sdf}}^2$ → high confidence
- $\text{SDF} > 0$: Point outside model → high $\sigma_{\text{sdf}}^2$ → low confidence
- $\text{SDF} < 0$: Point inside model → high $\sigma_{\text{sdf}}^2$ → very low confidence (inconsistent)

**Note:** The capture covariance is **fixed** once the point is stored. It represents the uncertainty at the moment of measurement, not an accumulated uncertainty.

### 5.3 Using Historical Points

When using a historical point at time $t$, we must account for **two sources of uncertainty**:

1. **Capture uncertainty** ($\Sigma_{\text{capture}}$): Fixed at storage time
2. **Current transformation uncertainty**: From room frame to robot frame

#### Why Covariance Composition is Required

Historical points are stored in the **room frame**, but the SDF optimization operates in the **robot frame**. The transformation from room to robot frame depends on the current robot pose, which has uncertainty $\Sigma_{\text{robot}}(t)$.

**Key insight:** The point position is fixed in the room frame, but our knowledge of *where that point is in the robot frame* depends on how well we know the current robot pose.

#### Transformation and Covariance Propagation

1. **Transform to robot frame:**
$$
\mathbf{p}_{\text{robot}}(t) = R(-r_\theta(t))^\top (\mathbf{p}_{\text{room}} - \mathbf{r}_{xy}(t))
$$

2. **Propagate covariance through the transformation:**
$$
\Sigma_{\text{total}} = \Sigma_{\text{capture}} + J(t) \cdot \Sigma_{\text{robot}}(t) \cdot J(t)^\top
$$

where $J(t)$ is the Jacobian of the room-to-robot transformation with respect to robot pose:

$$
J = \frac{\partial \mathbf{p}_{\text{robot}}}{\partial \mathbf{r}} = 
\begin{pmatrix}
-\cos(r_\theta) & -\sin(r_\theta) & -(p_y - r_y)\cos(r_\theta) + (p_x - r_x)\sin(r_\theta) \\
\sin(r_\theta) & -\cos(r_\theta) & -(p_y - r_y)\sin(r_\theta) - (p_x - r_x)\cos(r_\theta)
\end{pmatrix}
$$

3. **Compute weight for optimization:**
$$
w = \frac{1}{1 + \text{tr}(\Sigma_{\text{total}}) + \text{RFE}}
$$

#### Implications

| Robot state at capture ($t_0$) | Robot state now ($t$) | Point weight |
|-------------------------------|----------------------|--------------|
| Well localized (small $\Sigma$) | Well localized | **High** |
| Well localized | Poorly localized | Low |
| Poorly localized | Well localized | Low |
| Poorly localized | Poorly localized | **Very low** |

This ensures that historical points only contribute strongly when **both** the capture and current localization are reliable.

### 5.4 Remembered Free Energy (RFE)

Inspired by Expected Free Energy (EFE) for future planning, we introduce **Remembered Free Energy** for retrospective evidence evaluation:

$$
\text{RFE}_i = \sum_{\tau=t_0}^{t} \alpha^{t-\tau} \cdot w_\tau \cdot \text{SDF}(\mathbf{p}_i, \mathbf{s}_\tau)^2
$$

where:
- $\alpha \in (0,1)$: temporal decay factor
- $w_\tau = 1 / (1 + \text{tr}(\Sigma_{\text{robot}}(\tau)))$: weight based on robot certainty

**Interpretation:**
- $\text{RFE} \approx 0$: Point consistently on surface → **high evidence value**
- $\text{RFE} \gg 0$: Point inconsistent with model → **likely noise, low value**

### 5.5 Temporal Symmetry: EFE and RFE

Active Inference provides a principled framework for action selection via Expected Free Energy (EFE), which evaluates the quality of future policies. RFE complements this by evaluating the quality of past evidence.

| Concept | Direction | Question Answered |
|---------|-----------|-------------------|
| **EFE** | Future → Present | "What actions will reduce my uncertainty?" |
| **RFE** | Past → Present | "What evidence has been consistently reliable?" |

Together, they form a **temporally symmetric** view of inference:

$$
\text{Present belief} = f(\underbrace{\text{RFE-weighted past evidence}}_{\text{memory}}, \underbrace{\text{EFE-guided future actions}}_{\text{planning}})
$$

**Key insight:** Memory in Active Inference is not passive storage, but **evidence weighted by historical consistency and certainty**. Points with low RFE are "trusted memories" that anchor the belief, while points with high RFE are "unreliable memories" that are downweighted or forgotten.

### 5.6 RFE Integration with Uniform Surface Coverage

Historical points are organized in spatial bins for uniform surface coverage:
- **24 angular bins** around the box (XY plane)
- **8 height bins** (Z axis, 10cm each)
- **Maximum points per bin**: `max_historical / 192`

#### Quality Metric for Bin Selection

Within each bin, points compete based on a **quality score** that includes geometric information content:

$$
Q_i = \text{tr}(\Sigma_{\text{capture},i}) + \text{RFE}_i - \gamma \cdot E_i
$$

where $E_i \in [0, 1]$ is the **edge/corner score** and $\gamma$ is the edge bonus weight.

**Edge/Corner Detection:**

A point's edge score is computed by checking proximity to multiple box faces:

$$
E = \frac{\text{faces\_close} - 1}{2} + \epsilon_{\text{proximity}}
$$

where:
- `faces_close` = number of faces within threshold distance (0.05m)
- $\epsilon_{\text{proximity}}$ = exponential proximity bonus

| Location | Faces Close | Edge Score E | Geometric Value |
|----------|-------------|--------------|-----------------|
| Flat face | 1 | 0.0 | Low - redundant |
| Edge | 2 | 0.5 | Medium - constrains 2 dimensions |
| Corner | 3 | 1.0 | **High** - constrains all dimensions |

**Rationale:** Corner and edge points provide stronger constraints on box dimensions because they simultaneously touch multiple faces. A single corner observation constrains width, height, and depth, whereas a flat face point only confirms one dimension.

- **Lower Q = better point** (low uncertainty + low error + high geometric value)
- When a bin is full, new points replace existing points with higher Q

#### Effect of Quality Factors on Point Retention

| Σ_capture | RFE | Edge Score | Q | Outcome |
|-----------|-----|------------|---|---------|
| Low | Low | High (corner) | **Very Low** | **Kept** - best anchor |
| Low | Low | Low (flat) | Low | Kept - trusted |
| Low | High | High | Medium | May be replaced |
| High | Low | High | Medium | May be replaced |
| High | High | Low | **High** | **Replaced first** |

This ensures:
1. **Spatial uniformity**: Points cover all visible faces
2. **Temporal consistency**: Reliable points accumulate, noisy points are forgotten
3. **Capture quality**: Well-localized measurements are preferred

---

## 6. Complete Optimization Objective

### 6.1 Full Free Energy with Historical Points

$$
F(\mathbf{s}) = \underbrace{\frac{1}{2\sigma_o^2} \sum_{i=1}^{N_{\text{current}}} \text{SDF}(\mathbf{p}_i^{\text{curr}}, \mathbf{s})^2}_{\text{Current observations}}
+ \underbrace{\frac{1}{2} \sum_{j=1}^{N_{\text{hist}}} w_j \cdot \text{SDF}(\mathbf{p}_j^{\text{hist}}, \mathbf{s})^2}_{\text{Historical evidence}}
+ \underbrace{\frac{\lambda}{2} (\mathbf{s} - \bold{\mu}_{\text{prior}})^\top \Sigma_{\text{prior}}^{-1} (\mathbf{s} - \mathbf{\mu}_{\text{prior}})}_{\text{Prior regularization}}
$$

where:
- $w_j = 1 / (1 + \text{tr}(\Sigma_{\text{total},j}) + \text{RFE}_j)$: weight for historical point $j$
- $\lambda$: precision parameter balancing prior vs. likelihood

### 6.2 Gradient-Based Optimization

The posterior mode is found via gradient descent:

$$
\mathbf{s}^{(k+1)} = \mathbf{s}^{(k)} - \eta \nabla_{\mathbf{s}} F(\mathbf{s}^{(k)})
$$

Gradients are computed via automatic differentiation (PyTorch).

### 6.3 Posterior Covariance

After convergence, the posterior covariance is approximated by the inverse Hessian:

$$
\Sigma_{\text{post}}^{-1} = \nabla^2_{\mathbf{s}} F(\mathbf{s}) \big|_{\mathbf{s} = \mathbf{\mu}_{\text{post}}}
$$

---

## 7. Data Association

### 7.1 Clustering

LIDAR points are clustered using DBSCAN with parameters:
- $\epsilon$: neighborhood radius (meters)
- $\text{minPts}$: minimum points per cluster

### 7.2 Cost Matrix

For cluster $k$ and belief $j$, the association cost is:

$$
C_{kj} = \frac{1}{|\mathcal{C}_k|} \sum_{\mathbf{p} \in \mathcal{C}_k} \text{SDF}(\mathbf{p}, \mathbf{\mu}_j)^2
$$

### 7.3 Hungarian Algorithm

Optimal assignment minimizes total cost:

$$
\mathcal{A}^* = \arg\min_{\mathcal{A}} \sum_{(k,j) \in \mathcal{A}} C_{kj}
$$

subject to one-to-one matching constraints.

---

## 8. Belief Lifecycle

### 8.1 Initialization

New beliefs are created from unmatched clusters:
- Position: cluster centroid
- Dimensions: from cluster extent, regularized by prior $\mathcal{N}(0.5, 0.2^2)$
- Covariance: initial uncertainty values

### 8.2 Update

Matched beliefs are updated by minimizing VFE with current and historical observations.

### 8.3 Confidence Dynamics

$$
\kappa_{t+1} = \begin{cases}
\min(\kappa_t + \Delta\kappa, 1) & \text{if matched} \\
\gamma \cdot \kappa_t & \text{if unmatched}
\end{cases}
$$

where:
- $\Delta\kappa$: confidence boost on match
- $\gamma \in (0,1)$: decay factor

### 8.4 Removal

Beliefs with $\kappa < \kappa_{\text{threshold}}$ are removed from the active set.

---

## 9. Coordinate Frames

### 9.1 Frame Definitions

1. **Robot frame**: Origin at robot, X+ right, Y+ forward, Z+ up
2. **Room frame**: Origin at room center, fixed orientation
3. **Box frame**: Origin at box center, Y+ toward robot (dynamic)

### 9.2 Transformations

**Robot → Room:**
$$
\mathbf{p}_{\text{room}} = R(r_\theta) \mathbf{p}_{\text{robot}} + \begin{pmatrix} r_x \\ r_y \\ 0 \end{pmatrix}
$$

**Room → Robot:**
$$
\mathbf{p}_{\text{robot}} = R(-r_\theta)^\top (\mathbf{p}_{\text{room}} - \mathbf{r})
$$

---

## 10. Summary of Key Equations

| Concept | Equation |
|---------|----------|
| State vector | $\mathbf{s} = (c_x, c_y, w, h, d, \theta)^\top$ |
| SDF likelihood | $p(\mathbf{p} \mid \mathbf{s}) = \mathcal{N}(\text{SDF}(\mathbf{p}, \mathbf{s}) \mid 0, \sigma_o^2)$ |
| Free Energy | $F = -\ln p(\mathbf{o}, \mathbf{s}) = \text{likelihood} + \text{prior}$ |
| Capture covariance | $\Sigma_{\text{capture}} = \Sigma_{\text{robot}}(t_0) + \beta \cdot \text{SDF}^2 \cdot \mathbf{I}$ |
| Propagated covariance | $\Sigma_{\text{total}} = \Sigma_{\text{capture}} + J \Sigma_{\text{robot}}(t) J^\top$ |
| Point weight | $w = 1 / (1 + \text{tr}(\Sigma_{\text{total}}) + \text{RFE})$ |
| RFE | $\text{RFE} = \sum_\tau \alpha^{t-\tau} w_\tau \text{SDF}^2$ |

---

## 11. Implementation Notes

### 11.1 Numerical Stability

- SDF gradients can be unstable near corners; smoothing helps
- Covariance matrices must remain positive definite
- Use Cholesky decomposition for matrix inversions

### 11.2 Computational Efficiency

- Batch SDF computation on GPU (PyTorch CUDA)
- Limit historical points per belief (max 500)
- Uniform surface coverage via angular/height binning

### 11.3 Hyperparameters

| Parameter | Symbol | Typical Value |
|-----------|--------|---------------|
| Observation noise | $\sigma_o$ | 0.05 m |
| Process noise (position) | $\sigma_{xy}$ | 0.02 m |
| Process noise (size) | $\sigma_{\text{size}}$ | 0.01 m |
| Prior size mean | $\mu_{\text{size}}$ | 0.5 m |
| Prior size std | $\sigma_{\text{size}}$ | 0.2 m |
| Confidence decay | $\gamma$ | 0.85 |
| Prior precision | $\lambda$ | 0.5 |

---

## 12. References

1. Friston, K. (2010). The free-energy principle: a unified brain theory?
2. Lanillos, P. et al. (2021). Active Inference in Robotics and Artificial Agents
3. Da Costa, L. et al. (2022). Active Inference on Discrete State-Spaces

---

*Document version: 1.0*
*Last updated: February 2026*
